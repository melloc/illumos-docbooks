<chapter xml:id="zfsover-1"><title>ZFS File System (Introduction)</title>
<para>This chapter provides an overview of the ZFS file system and its features and benefits. This chapter also covers some basic terminology used throughout the rest of this book.</para>
<para>The following sections are provided in this chapter:</para>
<itemizedlist>
	<listitem><para><olink targetptr="zfsover-2" remap="internal">What Is ZFS?</olink></para></listitem>
	<listitem><para><olink targetptr="ftyue" remap="internal">ZFS Terminology</olink></para></listitem>
	<listitem><para><olink targetptr="gbcpt" remap="internal">ZFS Component Naming Requirements</olink></para></listitem>
</itemizedlist>
<sect1 xml:id="zfsover-2"><title>What Is ZFS?</title>
<para>The ZFS file system is a revolutionary new file system that fundamentally changes the way file systems are administered, with features and benefits not found in any other file system available today. ZFS has been designed to be robust, scalable, and simple to administer.</para>
<para>There are two implementations of ZFS filesystem currently: <trademark>Oracle Corp.</trademark> ZFS which is proprietary and as of now only available for <trademark>Oracle Solaris</trademark> operating system. Second implementation is maintained under the umbrella of OpenZFS Project. It is fully open source ZFS implementation, with the same codebase shared among illumos, *BSD and Linux operating systems. </para>
<para>This guide treats about OpenZFS filesystem. Whenever ZFS is mentioned, OpenZFS is meant. </para>
	<sect2 xml:id="gaypk"><title>ZFS Pooled Storage</title>
	<para>ZFS uses the concept of <emphasis>storage pools</emphasis> to manage physical storage. Historically, file systems were constructed on top of a single physical device. To address multiple devices and provide for data redundancy, the concept of a <emphasis>volume manager</emphasis> was introduced to provide the image of a single device so that file systems would not have to be modified to take advantage of multiple devices. This design added another layer of complexity and ultimately prevented certain file system advances, because the file system had no control over the physical placement of data on the virtualized volumes.</para>
	<para>ZFS eliminates the volume management altogether. Instead of forcing you to create virtualized volumes, ZFS aggregates devices into a storage pool. The storage pool describes the physical characteristics of the storage (device layout, data redundancy, and so on,) and acts as an arbitrary data store from which file systems can be created. File systems are no longer constrained to individual devices, allowing them to share space with all file systems in the pool. You no longer need to predetermine the size of a file system, as file systems grow automatically within the space allocated to the storage pool. When new storage is added, all file systems within the pool can immediately use the additional space without additional work. In many ways, the storage pool acts as a virtual memory system. When a memory DIMM is added to a system, the operating system doesn't force you to invoke some commands to configure the memory and assign it to individual processes. All processes on the system automatically use the additional memory.</para>
	</sect2>
	<sect2 xml:id="gaypi"><title>Transactional Semantics</title>
	<para>ZFS is a transactional file system, which means that the file system state is always consistent on disk. Traditional file systems overwrite data in place, which means that if the machine loses power, for example, between the time a data block is allocated and when it is linked into a directory, the file system will be left in an inconsistent state. Historically, this problem was solved through the use of the <command>fsck</command> command. This command was responsible for going through and verifying file system state, making an attempt to repair any inconsistencies in the process. This problem caused great pain to administrators and was never guaranteed to fix all possible problems. More recently, file systems have introduced the concept of <emphasis>journaling</emphasis>. The journaling process records action in a separate journal, which can then be replayed safely if a system crash occurs. This process introduces unnecessary overhead, because the data needs to be written twice, and often results in a new set of problems, such as when the journal can't be replayed properly.</para>
	<para>With a transactional file system, data is managed using <emphasis>copy on write</emphasis> semantics. Data is never overwritten, and any sequence of operations is either entirely committed or entirely ignored. This mechanism means that the file system can never be corrupted through accidental loss of power or a system crash. So, no need for a <command>fsck</command> equivalent exists. While the most recently written pieces of data might be lost, the file system itself will always be consistent. In addition, synchronous data (written using the <varname>O_DSYNC</varname> flag) is always guaranteed to be written before returning, so it is never lost.</para>
	</sect2>
	<sect2 xml:id="gaypb"><title>Checksums and Self-Healing Data</title>
	<para>With ZFS, all data and metadata is checksummed using a user-selectable algorithm. Traditional file systems that do provide checksumming have performed it on a per-block basis, out of necessity due to the volume management layer and traditional file system design. The traditional design means that certain failure modes, such as writing a complete block to an incorrect location, can result in properly checksummed data that is actually incorrect. ZFS checksums are stored in a way such that these failure modes are detected and can be recovered from gracefully. All checksumming and data recovery is done at the file system layer, and is transparent to applications.</para>
	<para>In addition, ZFS provides for self-healing data. ZFS supports storage pools with varying levels of data redundancy, including mirroring and a variation on RAID-5. When a bad data block is detected, ZFS fetches the correct data from another redundant copy, and repairs the bad data, replacing it with the good copy.</para>
	</sect2>
	<sect2 xml:id="gayou"><title>Unparalleled Scalability</title>
	<para>ZFS has been designed from the ground up to be the most scalable file system, ever. The file system itself is 128-bit, allowing for 256 quadrillion zettabytes of storage. All metadata is allocated dynamically, so no need exists to pre-allocate inodes or otherwise limit the scalability of the file system when it is first created. All the algorithms have been written with scalability in mind. Directories can have up to 2<superscript>48</superscript> (256 trillion) entries, and no limit exists on the number of file systems or number of filesthat can be contained within a file system.</para>
	</sect2>
	<sect2 xml:id="gbcbn"><title>ZFS Snapshots</title>
	<para>A <emphasis>snapshot</emphasis> is a read-only copy of a file system or volume. Snapshots can be created quickly and easily. Initially, snapshots consume no additional space within the pool.</para><para>As data within the active dataset changes, the snapshot consumes space by continuing to reference the old data. As a result, the snapshot prevents the data from being freed back to the pool.</para>
	</sect2>
	<sect2 xml:id="gayoc"><title>Simplified Administration</title>
	<para>Most importantly, ZFS provides a greatly simplified administration model. Through the use of hierarchical file system layout, property inheritance, and automanagement of mount points and NFS share semantics, ZFS makes it easy to create and manage file systems without needing multiple commands or editing configuration files. You can easily set quotas or reservations, turn compression on or off, or manage mount points for numerous file systems with a single command. Devices can be examined or repaired without having to understand a separate set of volume manager commands. You can take an unlimited number of instantaneous snapshots of file systems. You can backup and restore individual file systems.</para>
	<para>ZFS manages file systems through a hierarchy that allows for this simplified management of properties such as quotas, reservations, compression, and mount points. In this model, file systems become the central point of control. File systems themselves are very cheap (equivalent to a new directory), so you are encouraged to create a file system for each user, project, workspace, and so on. This design allows you to define fine-grained management points.</para>
	</sect2>
</sect1>
<sect1 xml:id="ftyue"><title>ZFS Terminology</title>
	<para>This section describes the basic terminology used throughout this book:</para>
	<variablelist>
		<varlistentry><term>checksum</term><listitem><para>A 256-bit hash of the data in a file system block. The checksum capability can range from the simple and fast fletcher2 (the default) to cryptographically strong hashes such as SHA256.</para></listitem></varlistentry>
		<varlistentry><term>clone</term><listitem><para>A file system whose initial contents are identical to the contents of a snapshot.</para><para>For information about clones, see <olink targetptr="gbcxz" remap="internal">Overview of ZFS Clones</olink>.</para></listitem></varlistentry>
		<varlistentry><term>dataset</term><listitem><para>A generic name for the following ZFS entities: clones, file systems, snapshots, or volumes.</para><para>Each dataset is identified by a unique name in the ZFS namespace. Datasets are identified using the following format:</para><para><replaceable>pool</replaceable>/<replaceable>path</replaceable>[<replaceable>@snapshot</replaceable>]</para><variablelist><varlistentry><term><replaceable>pool</replaceable></term><listitem><para>Identifies the name of the storage pool that contains the dataset</para></listitem></varlistentry>
		<varlistentry><term><replaceable>path</replaceable></term><listitem><para>Is a slash-delimited path name for the dataset object</para></listitem></varlistentry>
		<varlistentry><term><replaceable>snapshot</replaceable></term><listitem><para>Is an optional component that identifies a snapshot of a dataset</para></listitem></varlistentry>
	</variablelist><para>For more information about datasets, see <olink targetptr="gavwq" remap="internal">Chapter&nbsp;5,Managing ZFS File Systems</olink>.</para></listitem></varlistentry>
<varlistentry><term>file system</term><listitem><para>A dataset that contains a standard POSIX file system.</para>
<para>For more information about file systems, see <olink targetptr="gavwq" remap="internal">Chapter&nbsp;5,
Managing ZFS File Systems</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>mirror</term><listitem><para>A virtual device that stores identical copies of data on two
or more disks. If any disk in a mirror fails, any other disk in that mirror
can provide the same data.</para>
</listitem>
</varlistentry><varlistentry><term>pool</term><listitem><para>A logical group of devices describing the layout and physical
characteristics of the available storage. Space for datasets is allocated
from a pool.</para><para>For more information about storage pools, see <olink targetptr="gavwn" remap="internal">Chapter&nbsp;4,
Managing ZFS Storage Pools</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>RAID-Z</term><listitem><para>A virtual device that stores data and parity on multiple disks,
similar to RAID-5. For more information about
RAID-Z, see <olink targetptr="gamtu" remap="internal">RAID-Z Storage Pool Configuration</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>resilvering</term><listitem><para>The process of transferring data from one device to another
device is known as <emphasis>resilvering</emphasis>. For example, if a mirror
component is replaced or taken offline, the data from the up-to-date mirror
component is copied to the newly restored mirror component. This process is
referred to as <emphasis>mirror resynchronization</emphasis> in traditional
volume management products.</para><para>For more information about ZFS resilvering, see <olink targetptr="gbcus" remap="internal">Viewing
Resilvering Status</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>snapshot</term><listitem><para>A read-only image of a file system or volume at a given point
in time.</para><para>For more information about snapshots, see <olink targetptr="gbciq" remap="internal">Overview
of ZFS Snapshots</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>virtual device</term><listitem><para>A logical device in a pool, which can be a physical device,
a file, or a collection of devices.</para><para>For more information about virtual devices, see <olink targetptr="gazca" remap="internal">Identifying
Virtual Devices in a Storage Pool</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>volume</term><listitem><para>A dataset used to emulate a physical device. For example,
you can create an ZFS volume as a swap device.</para><para>For more information about ZFS volumes, see <olink targetptr="gaypf" remap="internal">ZFS
Volumes</olink>.</para>
</listitem>
</varlistentry>
</variablelist>
</sect1>
<sect1 xml:id="gbcpt"><title>ZFS Component Naming Requirements</title><para>Each ZFS component must be named according to the following rules:</para><itemizedlist><listitem><para>Empty components are not allowed.</para>
</listitem><listitem><para>Each component can only contain alphanumeric characters in
addition to the following four special characters:</para><itemizedlist><listitem><para>Underscore (_)</para>
</listitem><listitem><para>Hyphen (-)</para>
</listitem><listitem><para>Colon (:)</para>
</listitem><listitem><para>Period (.)</para>
</listitem>
</itemizedlist>
</listitem><listitem><para>Pool names must begin with a letter, except for the following
restrictions:</para><itemizedlist><listitem><para>The beginning sequence <literal>c</literal>[<literal>0-9</literal>]
is not allowed</para>
</listitem><listitem><para>The name <literal>log</literal> is reserved</para>
</listitem><listitem><para>A name that begins with <literal>mirror</literal>, <literal>raidz</literal>, or <literal>spare</literal> is not allowed because these name
are reserved.</para>
</listitem>
</itemizedlist><para>In addition, pool names must not contain a percent sign (<literal>%</literal>)</para>
</listitem><listitem><para>Dataset names must begin with an alphanumeric character. Dataset
names must not contain
a percent sign (<literal>%</literal>).</para>
</listitem>
</itemizedlist>
</sect1>
</chapter>
